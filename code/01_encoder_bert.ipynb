{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7c74fe53-3a77-487c-bf59-177f98d52642",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "yD6y2CEQtrNW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers import BertTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f8cb6f95-327e-4978-9007-b2a58a7f3167",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8IEM7wgDtrNX"
      },
      "outputs": [],
      "source": [
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "\n",
        "test = \"Every morning last summer in Greece, \" + \\\n",
        "       \"I visited the [MASK] where I would swim, \" + \\\n",
        "       \"play in the sand, and sunbathe.\"\n",
        "\n",
        "result = unmasker(test)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "aefc923e-c6c2-4646-8133-cf87f3442eb0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_OoutOxotrNX"
      },
      "source": [
        "## Sentiment Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "73c9f22d-68ad-479c-9e0d-226463455007",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "XHlkqEY7trNX"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.dropbox.com/scl/fi/i2esmtinb4qor0mzokybp/fed_sentiment_training.csv?rlkey=v9u7afunmy8w0v0lwizba5g25&dl=1'\n",
        "df = pd.read_csv(url, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c23d64a5-3f72-413d-b3bf-8a39ce6714e4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JwoMgHsEtrNX"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "04b240b0-9a91-44c4-b851-2976d4e58494",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "sIOrrwMetrNX"
      },
      "outputs": [],
      "source": [
        "df.loc[0, \"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cdb011eb-0c9e-4730-bbbb-ef4d90d44562",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "MEjfRSwttrNX"
      },
      "source": [
        "### Tokenizing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "08d976e9-196a-42e7-85c9-5c671fbdb5fb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "iR0yLMV_trNX"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b9d153b1-66da-4ef0-b077-07dc0eb55a51",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "K0pDTIvetrNX"
      },
      "outputs": [],
      "source": [
        "vocab = tokenizer.get_vocab()\n",
        "print(f\"Total number of tokens in vocabulary: {len(vocab)} \\n---------\")\n",
        "for _ in range(10):\n",
        "    word, idx = random.choice(list(vocab.items()))\n",
        "    print(word, idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f5ee257e-756d-451c-83c2-fd29e89b8ad9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "hEPOW3vLtrNX"
      },
      "outputs": [],
      "source": [
        "encoded_input1 = tokenizer(df.loc[0, \"text\"],\n",
        "                           max_length=100,\n",
        "                           padding=\"max_length\",\n",
        "                           return_tensors='pt')\n",
        "\n",
        "print(\"Tokens:\")\n",
        "temp_tokens = encoded_input1[\"input_ids\"][0]  # ← Add [0] here to get first sequence\n",
        "print(tokenizer.convert_ids_to_tokens(temp_tokens))\n",
        "print(\"\\n------------------------------------------\\n\")\n",
        "print(\"Tokens IDs:\")\n",
        "print(temp_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "dceb3e85-a502-4483-8eb0-794c25993392",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Dgku4SBstrNY"
      },
      "source": [
        "### Obtaining Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4cfebb8b-6e08-4970-9a8c-288d0f638229",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "gP5gxXMytrNY"
      },
      "outputs": [],
      "source": [
        "model = AutoModel.from_pretrained(\"bert-base-uncased\",\n",
        "                                  output_hidden_states=True,\n",
        "                                  output_attentions=True,\n",
        "                                  attn_implementation=\"eager\"\n",
        "                                  )\n",
        "\n",
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bb6d53ed-95f4-4281-b31f-e3b42b0b52a3",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ifn_3XNHtrNY"
      },
      "outputs": [],
      "source": [
        "# Step 1: Get BERT output without computing gradients (inference mode)\n",
        "with torch.no_grad():\n",
        "    result1 = model(**encoded_input1)\n",
        "\n",
        "# Step 2: Extract token-level embeddings from BERT's last layer\n",
        "last_hidden_state = result1.last_hidden_state\n",
        "print(f\"Token embeddings shape: {last_hidden_state.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Get attention mask (1 = real token, 0 = padding)\n",
        "attention_mask = encoded_input1[\"attention_mask\"]  # [1, 30]\n",
        "print(\"attention mask:\")\n",
        "print(attention_mask)\n",
        "print(\"\\n------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "kcuwBX65v5yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Zero out padding token embeddings\n",
        "# unsqueeze(-1) adds dimension: [1, 30] → [1, 30, 1]\n",
        "# This allows broadcasting when multiplying with embeddings [1, 30, 768]\n",
        "masked_embeddings = last_hidden_state * attention_mask.unsqueeze(-1)\n",
        "\n",
        "# Step 5: Compute mean pooling (average of non-padding tokens)\n",
        "# Numerator: sum all 30 token embeddings\n",
        "sum_embeddings = masked_embeddings.sum(dim=1)  # [1, 768]\n",
        "\n",
        "# Denominator: count how many real tokens\n",
        "num_real_tokens = attention_mask.sum(dim=1, keepdim=True)  # [1, 1]\n",
        "\n",
        "# Final sentence embedding: average of real token embeddings\n",
        "mean_embedding1 = sum_embeddings / num_real_tokens  # [1, 768]\n",
        "\n",
        "print(f\"Sentence embedding shape: {mean_embedding1.shape}\")\n",
        "print(\"\\n------------------------------------------\\n\")\n",
        "print(\"First Ten Elements of Embedding:\")\n",
        "print(mean_embedding1[0, :10])"
      ],
      "metadata": {
        "id": "97okzrbov-Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "628854c4-2e5c-4154-a778-8976c3c558ab",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ANszGqnUtrNY"
      },
      "outputs": [],
      "source": [
        "# %% Now scale to ALL examples in the dataset\n",
        "import numpy as np\n",
        "\n",
        "# Move model to GPU\n",
        "model = model.to('cuda')\n",
        "\n",
        "batch_size = 32  # Process 32 texts at once\n",
        "all_embeddings = []\n",
        "\n",
        "for i in range(0, len(df), batch_size):\n",
        "    batch_texts = df[\"text\"][i:i+batch_size].tolist()\n",
        "\n",
        "    # Tokenize the batch\n",
        "    encoded_input = tokenizer(batch_texts,\n",
        "                             max_length=30, # for speed\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True,\n",
        "                             return_tensors='pt').to('cuda')  # Move batch to GPU\n",
        "\n",
        "    with torch.no_grad():\n",
        "        result = model(**encoded_input)\n",
        "\n",
        "    last_hidden_state = result.last_hidden_state\n",
        "    attention_mask = encoded_input[\"attention_mask\"]\n",
        "    masked_embeddings = last_hidden_state * attention_mask.unsqueeze(-1)\n",
        "    mean_embedding = masked_embeddings.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
        "\n",
        "    # Move back to CPU and convert to numpy\n",
        "    batch_embeddings = mean_embedding.cpu().numpy()\n",
        "    all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "# stack into matrix\n",
        "embeddings_matrix = np.vstack(all_embeddings)\n",
        "\n",
        "print(f\"Embeddings matrix shape: {embeddings_matrix.shape}\")\n",
        "print(f\"Number of texts: {embeddings_matrix.shape[0]}\")\n",
        "print(f\"Embedding dimension: {embeddings_matrix.shape[1]}\")\n",
        "\n",
        "# Store in dataframe\n",
        "df['embedding'] = all_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "qCyNk56Ezviq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "4"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "02_encoder",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}