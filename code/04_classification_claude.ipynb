{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preliminaries"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-packages"
      },
      "outputs": [],
      "source": [
        "%pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from anthropic import Anthropic\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initialize-client"
      },
      "outputs": [],
      "source": [
        "# Initialize Anthropic client with API key from Colab secrets\n",
        "client = Anthropic(api_key=userdata.get(\"ANTHROPIC_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load-data"
      },
      "source": [
        "## Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-fed-data"
      },
      "outputs": [],
      "source": [
        "# Load the Fed sentiment training data\n",
        "url = 'https://www.dropbox.com/scl/fi/i2esmtinb4qor0mzokybp/fed_sentiment_training.csv?rlkey=v9u7afunmy8w0v0lwizba5g25&dl=1'\n",
        "df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "# Perform train-test split\n",
        "n = len(df)\n",
        "test_size = int(0.1 * n)\n",
        "indices = np.random.RandomState(95).permutation(n)\n",
        "train_idxs, test_idxs = indices[test_size:], indices[:test_size]\n",
        "\n",
        "df_test = df.iloc[test_idxs][[\"text\", \"sentiment\"]].copy()\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "classification-setup"
      },
      "source": [
        "## LLM Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helper-function"
      },
      "outputs": [],
      "source": [
        "# Helper function to call Claude API for classification\n",
        "def call_claude_classification(text, system_message=None, model=\"claude-sonnet-4-20250514\", temperature=0.0, max_tokens=100):\n",
        "    message_params = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Add system message if provided\n",
        "    if system_message:\n",
        "        message_params[\"system\"] = system_message\n",
        "\n",
        "    response = client.messages.create(**message_params)\n",
        "    return response.content[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "classify-function"
      },
      "outputs": [],
      "source": [
        "# Define system prompt\n",
        "system_prompt = \"\"\"You are a research assistant working for the Fed. You have a degree in Economics.\"\"\"\n",
        "\n",
        "def classify_text(texts):\n",
        "    predictions = []\n",
        "    total = len(texts)\n",
        "    \n",
        "    for i, text in enumerate(texts):\n",
        "        if i % 10 == 0 or i == total - 1:\n",
        "            print(f\"Processing {i+1}/{total}...\")\n",
        "        \n",
        "        user_prompt = f\"\"\"Your task is to classify the text into one of the three categories (\"dovish\", \"neutral\", \"hawkish\").\n",
        "        The text is taken at random from the texts of FOMC announcements.\n",
        "\n",
        "        IMPORTANT: Respond ONLY with valid JSON in this exact format, with no additional text or explanation:\n",
        "        {{\"category\": \"your_classification\"}}\n",
        "\n",
        "Text: {text}\"\"\"\n",
        "        \n",
        "        response = call_claude_classification(\n",
        "            user_prompt,\n",
        "            system_message=system_prompt,\n",
        "            temperature=0.0,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        \n",
        "        try:\n",
        "            # Remove markdown code blocks using regex\n",
        "            content = re.sub(r'^```json\\s*|\\s*```$', '', response.strip())\n",
        "            \n",
        "            result = json.loads(content)\n",
        "            predictions.append(result['category'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing response: {response}\")\n",
        "            predictions.append(None)\n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-classification"
      },
      "outputs": [],
      "source": [
        "# Apply to all test data\n",
        "df_test['llm_prediction'] = classify_text(df_test['text'].tolist())\n",
        "\n",
        "# Show results\n",
        "print(df_test[['sentiment', 'llm_prediction']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calculate-accuracy"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = (df_test['sentiment'] == df_test['llm_prediction']).mean()\n",
        "print(f\"\\nLLM Accuracy: {accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "confusion-matrix"
      },
      "outputs": [],
      "source": [
        "# Show confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(df_test['sentiment'], df_test['llm_prediction'], \n",
        "                      labels=['dovish', 'hawkish', 'neutral'])\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['dovish', 'hawkish', 'neutral'],\n",
        "            yticklabels=['dovish', 'hawkish', 'neutral'])\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('LLM Prediction')\n",
        "plt.title('LLM Confusion Matrix')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
