{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERWw--HwIf5r"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install-anthropic",
        "outputId": "c1683f82-900c-42d1-8b8f-90ef5ff1a1ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.72.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n",
            "Downloading anthropic-0.72.0-py3-none-any.whl (357 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.72.0\n"
          ]
        }
      ],
      "source": [
        "%pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7I2_eo5VIf5s"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from anthropic import Anthropic\n",
        "import json\n",
        "from IPython.display import display, HTML\n",
        "import warnings\n",
        "import requests\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z98s2ZpjIf5s"
      },
      "outputs": [],
      "source": [
        "# Initialize Anthropic client with API key from Colab secrets\n",
        "client = Anthropic(api_key=userdata.get(\"ANTHROPIC_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8xlA1BYIf5s"
      },
      "source": [
        "## Initial Call to Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UV_9URAXIf5s"
      },
      "outputs": [],
      "source": [
        "# Helper function to call Claude API\n",
        "# model → which Claude variant to use\n",
        "# temperature → randomness (0 = deterministic)\n",
        "# max_tokens → maximum length of the model’s reply\n",
        "# system_message → optional “persona” or behavior instructions\n",
        "def call_claude(prompt, system_message=None, model=\"claude-sonnet-4-20250514\", temperature=0.1, max_tokens=1500):\n",
        "    message_params = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Add system message if provided\n",
        "    if system_message:\n",
        "        message_params[\"system\"] = system_message\n",
        "\n",
        "    response = client.messages.create(**message_params)\n",
        "    return response.content[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TP0oTmc8If5s"
      },
      "outputs": [],
      "source": [
        "prompt = \"what is the most prestigious university in New York City?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JCwcd59kIf5s",
        "outputId": "3b060445-141a-4488-d718-04295a0ac0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CV2S3qNixJfonjV6mi34c'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2176301041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_claude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2333267027.py\u001b[0m in \u001b[0;36mcall_claude\u001b[0;34m(prompt, system_message, model, temperature, max_tokens)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmessage_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"system\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmessage_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    925\u001b[0m             )\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CV2S3qNixJfonjV6mi34c'}"
          ]
        }
      ],
      "source": [
        "response = call_claude(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQspZKbqIf5s"
      },
      "source": [
        "## Extracting Structured Representations of Unstructured Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnhiZasrIf5s"
      },
      "source": [
        "We begin by seeing how an LLM can represent the content of news articles. To begin, we use a New Yorker article about Trump's 2024 election victory. We'll load it from a URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YsPk92HFIf5t",
        "outputId": "3549323d-c3d3-4f9d-ba7a-4ffe5b66428f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Electing Donald J. Trump once could be dismissed as a fluke, an aberration, a terrible mistake—a consequential one, to be sure, yet still fundamentally an error. But America has now twice elected him as its President. It is a disastrous revelation about what the United States really is, as opposed to the country that so many hoped that it could be. His victory was a worst-case scenario—that a convicted felon, a chronic liar who mismanaged a deadly once-in-a-century pandemic, who tried to overtur\n"
          ]
        }
      ],
      "source": [
        "# https://www.newyorker.com/news/the-lede/donald-trump-wins-a-second-term\n",
        "\n",
        "# Load the example article from URL\n",
        "url = 'https://www.dropbox.com/scl/fi/6skmey1hnm68elfbkpyed/example_new_yorker.txt?rlkey=oeezh243buiauhpiv6fqhs9el&dl=1'\n",
        "\n",
        "response = requests.get(url)\n",
        "ny_article = response.text\n",
        "\n",
        "print(ny_article[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCkT2NsrIf5t"
      },
      "source": [
        "Extracting information from this raw text requires forming a prompt to the LLM. The study of how to effectively do so is called prompt engineering. For example:\n",
        "\n",
        "- OpenAI's guide is at https://platform.openai.com/docs/guides/prompt-engineering.\n",
        "- Anthropic's guide is https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQZGDePtIf5t"
      },
      "source": [
        "### \"Be clear and direct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WuVp3fa0If5t",
        "outputId": "ef0a8036-ebc0-4f5b-c220-efe934610ae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "whom does this article talk about?:\n",
            "\n",
            "Electing Donald J. Trump once could be dismissed as a fluke, a\n"
          ]
        }
      ],
      "source": [
        "# PRINCIPLE 1: Be clear and detailed (BAD PROMPT)\n",
        "prompt1 = f\"\"\"\n",
        "whom does this article talk about?:\n",
        "\n",
        "{ny_article}\n",
        "\"\"\"\n",
        "\n",
        "print(prompt1[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k5HeUE1iIf5t",
        "outputId": "8a5c585b-3984-4705-a427-6709a6367ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CV2S9ehTQG6emxiyrKT4n'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2307997892.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_claude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2333267027.py\u001b[0m in \u001b[0;36mcall_claude\u001b[0;34m(prompt, system_message, model, temperature, max_tokens)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmessage_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"system\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmessage_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    925\u001b[0m             )\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CV2S9ehTQG6emxiyrKT4n'}"
          ]
        }
      ],
      "source": [
        "response = call_claude(prompt1, temperature=0.0, max_tokens=5000)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUzV4hsnIf5t"
      },
      "outputs": [],
      "source": [
        "# BETTER PROMPT - More specific and structured\n",
        "prompt2 = f\"\"\"\n",
        "We want to extract the relevant people from a news article.\n",
        "\n",
        "Please follow these steps:\n",
        "1. Identify all the people mentioned and any description of them\n",
        "2. Identify any political offices mentioned\n",
        "\n",
        "Here is the text of the article:\n",
        "{ny_article}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMKJYFRrIf5t"
      },
      "outputs": [],
      "source": [
        "response = call_claude(prompt2, temperature=0.0, max_tokens=5000)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7silJ414If5t"
      },
      "source": [
        "Modern LLMs also support prompts that directly incorporate data schema which can help clarify and organize what information you want. Some even have an explicit function mode that guarantees a particular output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYvttF5rIf5t"
      },
      "outputs": [],
      "source": [
        "# STRUCTURED JSON OUTPUT\n",
        "prompt3 = f\"\"\"\n",
        "We want to extract the relevant characters from a news article.\n",
        "\n",
        "Please provide your output in the following JSON format:\n",
        "\n",
        "{{\n",
        "  \"people\": [\n",
        "    {{\n",
        "      \"name\": \"person's full name\",\n",
        "      \"description\": \"their role or description from the article\"\n",
        "    }}\n",
        "  ],\n",
        "  \"institutions\": [\n",
        "    {{\n",
        "      \"name\": \"institution name\",\n",
        "      \"type\": \"type of institution (e.g., government, media, party, etc.)\",\n",
        "      \"context\": \"brief context of how it's mentioned\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Here is the text of the article:\n",
        "{ny_article}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9ESv6kKIf5t"
      },
      "outputs": [],
      "source": [
        "response = call_claude(prompt3, temperature=0.0, max_tokens=5000)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWfp5pyRIf5t"
      },
      "source": [
        "These data schema can fundamentally change how input text is represented: temporal, network, etc. Below we illustrate a network-structured prompt and associated visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mj2HLG3If5t"
      },
      "outputs": [],
      "source": [
        "# NETWORK GRAPH REPRESENTATION\n",
        "prompt4 = f\"\"\"\n",
        "Create a network graph representation of the article.\n",
        "\n",
        "Return JSON:\n",
        "\n",
        "{{\n",
        "  \"nodes\": [\n",
        "    {{\"id\": \"trump\", \"label\": \"Donald Trump\", \"type\": \"person\"}},\n",
        "    {{\"id\": \"gop\", \"label\": \"Republican Party\", \"type\": \"institution\"}}\n",
        "  ],\n",
        "  \"edges\": [\n",
        "    {{\"from\": \"trump\", \"to\": \"gop\", \"relationship\": \"leads\"}},\n",
        "    {{\"from\": \"trump\", \"to\": \"harris\", \"relationship\": \"defeated\"}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Article: {ny_article}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrNTnBDoIf5t"
      },
      "outputs": [],
      "source": [
        "response = call_claude(prompt4, temperature=0.0, max_tokens=5000)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uh0o-cVIf5t"
      },
      "source": [
        "### System Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDOaEmFXIf5t"
      },
      "source": [
        "A system prompt allows you to endow the LLM with a \"persona\" which guides what output is generated. We'll illustrate this with a sample article about Venezuelan gangs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYS556hxIf5t"
      },
      "outputs": [],
      "source": [
        "# https://www.texastribune.org/2024/09/18/texas-venezuelan-gang-tren-de-aragua-abbott-crackdown/\n",
        "\n",
        "# Load the example article from URL\n",
        "url = 'https://www.dropbox.com/scl/fi/bgkrdghzoz3xzhcyiw2ay/example_texas_tribune.txt?rlkey=9xn97iyovumfnbw9kyevvt9b4&dl=1'\n",
        "\n",
        "response = requests.get(url)\n",
        "tt_article = response.text\n",
        "\n",
        "print(tt_article[:500])\n",
        "\n",
        "my_prompt = f\"\"\"Summarize the article below.\n",
        "Article:\n",
        "{tt_article}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQkunJZRIf5t"
      },
      "outputs": [],
      "source": [
        "# System prompt example 1: Constrain format\n",
        "system_message1 = \"You are a helpful assistant that replies with a concise one-sentence answer that always starts with the letter T.\"\n",
        "\n",
        "response = call_claude(my_prompt, system_message=system_message1, temperature=0.0, max_tokens=100)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHHjD96oIf5t"
      },
      "outputs": [],
      "source": [
        "# System prompt example 2: Content filtering\n",
        "system_message2 = \"\"\"You are a language model that works with young children.\n",
        "Never produce content related to violence or gangs.\n",
        "If asked to produce this content please reply with the phrase 'I can't do that :( \\nViolence is not good'\"\"\"\n",
        "\n",
        "response = call_claude(my_prompt, system_message=system_message2, temperature=0.0, max_tokens=100)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU16yt8IrRc1"
      },
      "source": [
        "### Image Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYNe-A1drRc1"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "# Read and encode the PDF\n",
        "pdf_url = \"https://www.dropbox.com/scl/fi/pvf3yluu4i2ymcze9ngzx/invoice_example.pdf?rlkey=8a2b57gcksthwrulfvwpv5955&dl=1\"\n",
        "\n",
        "# Download the PDF content\n",
        "response = requests.get(pdf_url)\n",
        "pdf_data = base64.standard_b64encode(response.content).decode(\"utf-8\")\n",
        "\n",
        "# Create the extraction prompt\n",
        "prompt = \"\"\"Please extract the following information from this receipt and return it as a JSON object:\n",
        "\n",
        "{\n",
        "  \"vendor_name\": \"name of the business\",\n",
        "  \"date\": \"date in YYYY-MM-DD format\",\n",
        "  \"time\": \"time if available\",\n",
        "  \"total_amount\": \"total amount as a number\",\n",
        "  \"currency\": \"currency code\",\n",
        "  \"tax_amount\": \"tax amount if shown\",\n",
        "  \"payment_method\": \"payment method if indicated\",\n",
        "  \"receipt_number\": \"receipt or invoice number if available\",\n",
        "  \"items\": [\"list of items/services\"],\n",
        "  \"vendor_address\": \"full address if available\",\n",
        "  \"vendor_phone\": \"phone number if available\",\n",
        "  \"additional_info\": \"any other relevant information\"\n",
        "}\n",
        "\n",
        "CRITICAL: Your response must contain ONLY valid JSON. Do not include any markdown formatting, code blocks, or text outside the JSON object. Start your response with { and end with }.\"\"\"\n",
        "\n",
        "# Make the API call with PDF document\n",
        "response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=1000,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"document\",\n",
        "                    \"source\": {\n",
        "                        \"type\": \"base64\",\n",
        "                        \"media_type\": \"application/pdf\",\n",
        "                        \"data\": pdf_data,\n",
        "                    },\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Extract and clean the response\n",
        "response_text = response.content[0].text\n",
        "\n",
        "# Clean up any markdown formatting\n",
        "cleaned_response = response_text.strip()\n",
        "if cleaned_response.startswith(\"```json\"):\n",
        "    cleaned_response = cleaned_response[7:]\n",
        "if cleaned_response.startswith(\"```\"):\n",
        "    cleaned_response = cleaned_response[3:]\n",
        "if cleaned_response.endswith(\"```\"):\n",
        "    cleaned_response = cleaned_response[:-3]\n",
        "cleaned_response = cleaned_response.strip()\n",
        "\n",
        "# Parse and display the JSON\n",
        "receipt_data = json.loads(cleaned_response)\n",
        "print(json.dumps(receipt_data, indent=2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}