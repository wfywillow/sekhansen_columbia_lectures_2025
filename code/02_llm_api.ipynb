{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERWw--HwIf5r"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-anthropic"
      },
      "outputs": [],
      "source": [
        "%pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I2_eo5VIf5s"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from anthropic import Anthropic\n",
        "import json\n",
        "from IPython.display import display, HTML\n",
        "import warnings\n",
        "import requests\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z98s2ZpjIf5s"
      },
      "outputs": [],
      "source": [
        "# Initialize Anthropic client with API key from Colab secrets\n",
        "client = Anthropic(api_key=userdata.get(\"ANTHROPIC_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8xlA1BYIf5s"
      },
      "source": [
        "## Initial Call to Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV_9URAXIf5s"
      },
      "outputs": [],
      "source": [
        "# Helper function to call Claude API\n",
        "def call_claude(prompt, system_message=None, model=\"claude-sonnet-4-20250514\", temperature=0.1, max_tokens=1500):\n",
        "    message_params = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Add system message if provided\n",
        "    if system_message:\n",
        "        message_params[\"system\"] = system_message\n",
        "\n",
        "    response = client.messages.create(**message_params)\n",
        "    return response.content[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP0oTmc8If5s"
      },
      "outputs": [],
      "source": [
        "prompt = \"what is the most prestigious university in New York City?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCwcd59kIf5s"
      },
      "outputs": [],
      "source": [
        "response = call_claude(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQspZKbqIf5s"
      },
      "source": [
        "## Extracting Structured Representations of Unstructured Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnhiZasrIf5s"
      },
      "source": [
        "We begin by seeing how an LLM can represent the content of news articles. To begin, we use a New Yorker article about Trump's 2024 election victory. We'll load it from a URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsPk92HFIf5t"
      },
      "outputs": [],
      "source": [
        "# https://www.newyorker.com/news/the-lede/donald-trump-wins-a-second-term\n",
        "\n",
        "# Load the example article from URL\n",
        "url = 'https://www.dropbox.com/scl/fi/6skmey1hnm68elfbkpyed/example_new_yorker.txt?rlkey=oeezh243buiauhpiv6fqhs9el&dl=1'\n",
        "\n",
        "response = requests.get(url)\n",
        "ny_article = response.text\n",
        "\n",
        "print(ny_article[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCkT2NsrIf5t"
      },
      "source": [
        "Extracting information from this raw text requires forming a prompt to the LLM. The study of how to effectively do so is called prompt engineering. For example:\n",
        "\n",
        "- OpenAI's guide is at https://platform.openai.com/docs/guides/prompt-engineering.\n",
        "- Anthropic's guide is https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQZGDePtIf5t"
      },
      "source": [
        "### \"Be clear and direct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuVp3fa0If5t"
      },
      "outputs": [],
      "source": [
        "# PRINCIPLE 1: Be clear and detailed (BAD PROMPT)\n",
        "prompt1 = f\"\"\"\n",
        "whom does this article talk about?:\n",
        "\n",
        "{ny_article}\n",
        "\"\"\"\n",
        "\n",
        "print(prompt1[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5HeUE1iIf5t"
      },
      "outputs": [],
      "source": [
        "response = call_claude(prompt1, temperature=0.0, max_tokens=5000)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUzV4hsnIf5t"
      },
      "outputs": [],
      "source": [
        "# BETTER PROMPT - More specific and structured\n",
        "prompt2 = f\"\"\"\n",
        "We want to extract the relevant people from a news article.\n",
        "\n",
        "Please follow these steps:\n",
        "1. Identify all the people mentioned and any description of them\n",
        "2. Identify any political offices mentioned\n",
        "\n",
        "Here is the text of the article:\n",
        "{ny_article}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMKJYFRrIf5t"
      },
      "outputs": [],
      "source": [
        "response = call_claude(prompt2, temperature=0.0, max_tokens=5000)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7silJ414If5t"
      },
      "source": [
        "Modern LLMs also support prompts that directly incorporate data schema which can help clarify and organize what information you want. Some even have an explicit function mode that guarantees a particular output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYvttF5rIf5t"
      },
      "outputs": [],
      "source": [
        "# STRUCTURED JSON OUTPUT\n",
        "prompt3 = f\"\"\"\n",
        "We want to extract the relevant characters from a news article.\n",
        "\n",
        "Please provide your output in the following JSON format:\n",
        "\n",
        "{{\n",
        "  \"people\": [\n",
        "    {{\n",
        "      \"name\": \"person's full name\",\n",
        "      \"description\": \"their role or description from the article\"\n",
        "    }}\n",
        "  ],\n",
        "  \"institutions\": [\n",
        "    {{\n",
        "      \"name\": \"institution name\",\n",
        "      \"type\": \"type of institution (e.g., government, media, party, etc.)\",\n",
        "      \"context\": \"brief context of how it's mentioned\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Here is the text of the article:\n",
        "{ny_article}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9ESv6kKIf5t"
      },
      "outputs": [],
      "source": [
        "response = call_claude(prompt3, temperature=0.0, max_tokens=5000)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWfp5pyRIf5t"
      },
      "source": [
        "These data schema can fundamentally change how input text is represented: temporal, network, etc. Below we illustrate a network-structured prompt and associated visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mj2HLG3If5t"
      },
      "outputs": [],
      "source": [
        "# NETWORK GRAPH REPRESENTATION\n",
        "prompt4 = f\"\"\"\n",
        "Create a network graph representation of the article.\n",
        "\n",
        "Return JSON:\n",
        "\n",
        "{{\n",
        "  \"nodes\": [\n",
        "    {{\"id\": \"trump\", \"label\": \"Donald Trump\", \"type\": \"person\"}},\n",
        "    {{\"id\": \"gop\", \"label\": \"Republican Party\", \"type\": \"institution\"}}\n",
        "  ],\n",
        "  \"edges\": [\n",
        "    {{\"from\": \"trump\", \"to\": \"gop\", \"relationship\": \"leads\"}},\n",
        "    {{\"from\": \"trump\", \"to\": \"harris\", \"relationship\": \"defeated\"}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Article: {ny_article}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrNTnBDoIf5t"
      },
      "outputs": [],
      "source": [
        "response = call_claude(prompt4, temperature=0.0, max_tokens=5000)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uh0o-cVIf5t"
      },
      "source": [
        "### System Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDOaEmFXIf5t"
      },
      "source": [
        "A system prompt allows you to endow the LLM with a \"persona\" which guides what output is generated. We'll illustrate this with a sample article about Venezuelan gangs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYS556hxIf5t"
      },
      "outputs": [],
      "source": [
        "# https://www.texastribune.org/2024/09/18/texas-venezuelan-gang-tren-de-aragua-abbott-crackdown/\n",
        "\n",
        "# Load the example article from URL\n",
        "url = 'https://www.dropbox.com/scl/fi/bgkrdghzoz3xzhcyiw2ay/example_texas_tribune.txt?rlkey=9xn97iyovumfnbw9kyevvt9b4&dl=1'\n",
        "\n",
        "response = requests.get(url)\n",
        "tt_article = response.text\n",
        "\n",
        "print(tt_article[:500])\n",
        "\n",
        "my_prompt = f\"\"\"Summarize the article below.\n",
        "Article:\n",
        "{tt_article}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQkunJZRIf5t"
      },
      "outputs": [],
      "source": [
        "# System prompt example 1: Constrain format\n",
        "system_message1 = \"You are a helpful assistant that replies with a concise one-sentence answer that always starts with the letter T.\"\n",
        "\n",
        "response = call_claude(my_prompt, system_message=system_message1, temperature=0.0, max_tokens=100)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHHjD96oIf5t"
      },
      "outputs": [],
      "source": [
        "# System prompt example 2: Content filtering\n",
        "system_message2 = \"\"\"You are a language model that works with young children.\n",
        "Never produce content related to violence or gangs.\n",
        "If asked to produce this content please reply with the phrase 'I can't do that :( \\nViolence is not good'\"\"\"\n",
        "\n",
        "response = call_claude(my_prompt, system_message=system_message2, temperature=0.0, max_tokens=100)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU16yt8IrRc1"
      },
      "source": [
        "### Image Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYNe-A1drRc1"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "# Read and encode the PDF\n",
        "pdf_url = \"https://www.dropbox.com/scl/fi/pvf3yluu4i2ymcze9ngzx/invoice_example.pdf?rlkey=8a2b57gcksthwrulfvwpv5955&dl=1\"\n",
        "\n",
        "# Download the PDF content\n",
        "response = requests.get(pdf_url)\n",
        "pdf_data = base64.standard_b64encode(response.content).decode(\"utf-8\")\n",
        "\n",
        "# Create the extraction prompt\n",
        "prompt = \"\"\"Please extract the following information from this receipt and return it as a JSON object:\n",
        "\n",
        "{\n",
        "  \"vendor_name\": \"name of the business\",\n",
        "  \"date\": \"date in YYYY-MM-DD format\",\n",
        "  \"time\": \"time if available\",\n",
        "  \"total_amount\": \"total amount as a number\",\n",
        "  \"currency\": \"currency code\",\n",
        "  \"tax_amount\": \"tax amount if shown\",\n",
        "  \"payment_method\": \"payment method if indicated\",\n",
        "  \"receipt_number\": \"receipt or invoice number if available\",\n",
        "  \"items\": [\"list of items/services\"],\n",
        "  \"vendor_address\": \"full address if available\",\n",
        "  \"vendor_phone\": \"phone number if available\",\n",
        "  \"additional_info\": \"any other relevant information\"\n",
        "}\n",
        "\n",
        "CRITICAL: Your response must contain ONLY valid JSON. Do not include any markdown formatting, code blocks, or text outside the JSON object. Start your response with { and end with }.\"\"\"\n",
        "\n",
        "# Make the API call with PDF document\n",
        "response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=1000,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"document\",\n",
        "                    \"source\": {\n",
        "                        \"type\": \"base64\",\n",
        "                        \"media_type\": \"application/pdf\",\n",
        "                        \"data\": pdf_data,\n",
        "                    },\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Extract and clean the response\n",
        "response_text = response.content[0].text\n",
        "\n",
        "# Clean up any markdown formatting\n",
        "cleaned_response = response_text.strip()\n",
        "if cleaned_response.startswith(\"```json\"):\n",
        "    cleaned_response = cleaned_response[7:]\n",
        "if cleaned_response.startswith(\"```\"):\n",
        "    cleaned_response = cleaned_response[3:]\n",
        "if cleaned_response.endswith(\"```\"):\n",
        "    cleaned_response = cleaned_response[:-3]\n",
        "cleaned_response = cleaned_response.strip()\n",
        "\n",
        "# Parse and display the JSON\n",
        "receipt_data = json.loads(cleaned_response)\n",
        "print(json.dumps(receipt_data, indent=2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}